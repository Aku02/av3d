# Copyright (c) Meta Platforms, Inc. and affiliates.

# Same as the evaluation_backup different sampling strategy!
import logging
import os

import imageio
import numpy as np
import torch
from tava.utils.structures import namedtuple_map, Rays
from tava.utils.training import compute_psnr, compute_ssim
from tqdm import tqdm

LOGGER = logging.getLogger(__name__)


@torch.no_grad()
def eval_epoch(
    model,
    dataset,
    data_preprocess_func,
    render_every: int = 1,
    test_chunk: int = 1024,
    save_dir: str = None,
    local_rank: int = 0,
    world_size: int = 1,
    step: int = None,
):
    """The multi-gpu evaluation function."""
    device = "cuda:%d" % local_rank
    metrics = {
        "psnr": torch.tensor(0.0, device=device),
        "ssim": torch.tensor(0.0, device=device),
    }

    if world_size > 1:
        # sync across all GPUs
        torch.distributed.barrier(device_ids=[local_rank])

    model.eval()
    model = model.module if hasattr(model, "module") else model

    if save_dir is not None:
        os.makedirs(save_dir, exist_ok=True)

    # split tasks across gpus
    index_list_all = list(range(len(dataset)))[::render_every]
    index_list = index_list_all[local_rank::world_size]
    for i, index in enumerate(index_list):
        LOGGER.info(
            "Processing %d/%d in Rank %d!"
            % (i + 1, len(index_list), local_rank)
        )

        data = dataset[index]
        data = data_preprocess_func(data)
        rays = data.pop("rays")
        pixels = data.pop("pixels")
        # verts = data.pop("vertices")

        # forward
        pred_color, pred_depth, pred_acc, pred_warp = render_image(
            model=model,
            rays=rays,
            randomized=False,
            normalize_disp=False,
            chunk=test_chunk,
            # verts = verts,
            **data,
        )
        pred_warp = pred_warp * (pred_acc > 0.001)

        # psnr & ssim
        psnr = compute_psnr(pred_color, pixels, mask=None)
        ssim = compute_ssim(pred_color, pixels, mask=None)
        metrics["psnr"] += psnr
        metrics["ssim"] += ssim

        # save images
        if save_dir is not None:
            img_to_save = torch.cat([pred_color, pixels], dim=1)
            # img_to_save = pred_color#torch.cat([pred_color], dim=1)
            
            sid, meta_id, cid = (
                data.get("subject_id", ""),
                data.get("meta_id", ""),
                data.get("camera_id", ""),
            )
            image_path = os.path.join(
                save_dir, f"{index:04d}_{sid}_{meta_id}_{cid}_{step}.png"
            )
            imageio.imwrite(
                image_path,
                np.uint8(img_to_save.cpu().numpy() * 255.0),
            )
            imageio.imwrite(
                image_path.replace(".png", "_mask.png"),
                np.uint8(pred_acc.cpu().numpy() * 255.0),
            )
            # if pred_warp.shape[-1] == 3:
            #     imageio.imwrite(
            #         image_path.replace(".png", ".exr"),
            #         np.float32(pred_warp.cpu().numpy()),
            #     )
            # else:
            np.save(
                image_path.replace(".png", ".npy"),
                np.float32(pred_warp.cpu().numpy()),
            )

    if world_size > 1:
        # sync across all GPUs
        torch.distributed.barrier(device_ids=[local_rank])
        for key in metrics.keys():
            torch.distributed.all_reduce(
                metrics[key], op=torch.distributed.ReduceOp.SUM
            )
        torch.distributed.barrier(device_ids=[local_rank])

    for key, value in metrics.items():
        metrics[key] = value / len(index_list_all)
    return metrics

def patch_sampler(image, height, width, patch_size = 64):
    patch_grid = []
    for y in range(0, height, patch_size):
        for x in range(0, width, patch_size):
            patch = image[y:y+patch_size, x:x+patch_size]
            patch_grid.append(patch)
    return patch_grid

def join_patches(patch_grid, dim, device,height = 1024, width=1024, patch_size=64):
    # Calculate the number of patches in each dimension
    num_patches_h = height // patch_size
    num_patches_w = width // patch_size

    # Create an empty image with the original size
    if dim == 0:
        reconstructed_image = torch.zeros((height, width)).to(device)
    else:    
        reconstructed_image = torch.zeros((height, width, dim)).to(device)

    # Iterate over the patch grid and place each patch in the correct position
    for i, patch in enumerate(patch_grid):
        # Calculate the row and column indices of the current patch
        row_idx = i // num_patches_w
        col_idx = i % num_patches_w

        # Calculate the starting position of the current patch in the reconstructed image
        start_row = row_idx * patch_size
        start_col = col_idx * patch_size

        # Place the current patch in the correct position within the reconstructed image
        if dim == 0:
            reconstructed_image[start_row:start_row+patch_size, start_col:start_col+patch_size] = patch
        else:
            reconstructed_image[start_row:start_row+patch_size, start_col:start_col+patch_size, :] = patch

    return reconstructed_image

@torch.no_grad()
def render_image(model, rays, chunk=8192, verts = None, **kwargs):
    """Render all the pixels of an image (in test mode).

    Args:
      model: the model of nerf.
      rays: a `Rays` namedtuple, the rays to be rendered.
      chunk: int, the size of chunks to render sequentially.

    Returns:
      rgb: torch.tensor, rendered color image.
      disp: torch.tensor, rendered disparity image.
      acc: torch.tensor, rendered accumulated weights per pixel.
      warp: torch.tensor, correspondance per pixel.
    """
    height, width = rays[0].shape[:2]
    num_rays = height * width
    
    # breakpoint()
    dummy = [rays.origins, rays.directions, rays.viewdirs, rays.radii]
    ray_patches = []
    for dum in dummy:
        patches = patch_sampler(dum, height, width)
        ray_patches.append(patches)

    rays_origins = torch.cat([patch.unsqueeze(0) for patch in ray_patches[0]], dim=0)
    rays_directions = torch.cat([patch.unsqueeze(0) for patch in ray_patches[1]], dim=0)
    rays_viewdirs = torch.cat([patch.unsqueeze(0) for patch in ray_patches[2]], dim=0)
    rays_radii = torch.cat([patch.unsqueeze(0) for patch in ray_patches[3]], dim=0)
    
    rays  = Rays(
        origins=rays_origins,#.view(-1, 3),  # [n_cams, height, width, 3]
        directions=rays_directions,#.view(-1,3),  # [n_cams, height, width, 3]
        viewdirs=rays_viewdirs,#.view(-1,3),  # [n_cams, height, width, 3]
        radii=rays_radii,#.view(-1,1),  # [n_cams, height, width, 1]
        # near far is not needed when they are estimated by skeleton.
        near=None,
        far=None,
    )
    
    # rays = namedtuple_map(lambda r: r.reshape([num_rays] + list(r.shape[2:])), rays)
    results = []
    chunk = 2
    for i in tqdm(range(0, 256, chunk)):
        chunk_rays = namedtuple_map(lambda r: r[i : i + chunk, :,:,:], rays)
        chunk_rays = namedtuple_map(lambda r: r.view(2*64*64,-1), chunk_rays)
        # breakpoint()
        chunk_results = model(rays=chunk_rays,vert = verts, **kwargs)[0][-1]
        results.append(chunk_results[0:5])
        
        # torch.cuda.empty_cache()
    rgb, depth, acc, warp, feat = [torch.cat(r, dim=0) for r in zip(*results)]
    # breakpoint()
    rgb = join_patches(rgb, 3, rgb.device)
    depth = join_patches(depth, 0, rgb.device)
    acc = join_patches(acc, 0, rgb.device)
    warp = join_patches(warp, 3, rgb.device)
    feat = join_patches(feat,16, rgb.device)
    
    
    return (
        rgb.view((height, width, -1)),
        depth.view((height, width, -1)),
        acc.view((height, width, -1)),
        warp.view((height, width, -1)),
    )
